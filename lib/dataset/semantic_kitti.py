import os.path as osp
import pickle

import numpy as np
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms as T

from lib.utils.augmentation_3d import augment_and_scale_3d
from lib.utils.refine_pseudo_labels import refine_pseudo_labels


def sample_lidar_lines(
    depth_map: np.ndarray, intrinsics: np.ndarray, keep_ratio: float = 1.0
) -> np.ndarray:
    """
    Takes in input a depth map generated by a 64 line lidar and sparsify the number of
    lines used, returning a sparse depth map with less lidar lines.
    Parameters
    ----------
    depth_map: array like
        sparse depth map of shape H x W x 1
    intrinsics: array like
        the intrinsic parameters of shape 3 x 3
    keep_ratio: float, default 1.0
        the sparsification parameter, 1.0 is 64 lines, 0.50 roughly 32 lines and so on.
    Returns
    -------
    sparse_depth_map: array like
        the sparsified depth map of shape H x W x 1
    """

    v, u, _ = np.nonzero(depth_map)
    z = depth_map[v, u, 0]
    points = np.linalg.inv(intrinsics) @ (np.vstack([u, v, np.ones_like(u)]) * z)
    points = points.transpose([1, 0])

    scan_y = points[:, 1]
    distance = np.linalg.norm(points, 2, axis=1)
    pitch = np.arcsin(scan_y / distance)
    num_points = np.shape(pitch)[0]
    pitch = np.reshape(pitch, (num_points, 1))

    max_pitch = np.max(pitch)
    min_pitch = np.min(pitch)
    angle_interval = (max_pitch - min_pitch) / 64.0
    angle_label = np.round((pitch - min_pitch) / angle_interval)
    sampling_mask = angle_label % (1.0 / keep_ratio) == 0

    final_mask = np.zeros_like(depth_map, dtype=bool)
    final_mask[depth_map[..., 0] > 0] = sampling_mask
    sampled_depth = np.zeros_like(final_mask, dtype=np.float32)
    sampled_depth[final_mask] = depth_map[final_mask]
    return sampled_depth


class SemanticKITTIBase(Dataset):
    """SemanticKITTI dataset"""

    # https://github.com/PRBonn/semantic-kitti-api/blob/master/config/semantic-kitti.yaml
    id_to_class_name = {
        0: "unlabeled",
        1: "outlier",
        10: "car",
        11: "bicycle",
        13: "bus",
        15: "motorcycle",
        16: "on-rails",
        18: "truck",
        20: "other-vehicle",
        30: "person",
        31: "bicyclist",
        32: "motorcyclist",
        40: "road",
        44: "parking",
        48: "sidewalk",
        49: "other-ground",
        50: "building",
        51: "fence",
        52: "other-structure",
        60: "lane-marking",
        70: "vegetation",
        71: "trunk",
        72: "terrain",
        80: "pole",
        81: "traffic-sign",
        99: "other-object",
        252: "moving-car",
        253: "moving-bicyclist",
        254: "moving-person",
        255: "moving-motorcyclist",
        256: "moving-on-rails",
        257: "moving-bus",
        258: "moving-truck",
        259: "moving-other-vehicle",
    }

    class_name_to_id = {v: k for k, v in id_to_class_name.items()}

    # use those categories if merge_classes == True (common with A2D2)
    categories = {
        "car": ["car", "moving-car"],
        "truck": ["truck", "moving-truck"],
        "bike": [
            "bicycle",
            "motorcycle",
            "bicyclist",
            "motorcyclist",
            "moving-bicyclist",
            "moving-motorcyclist",
        ],  # riders are labeled as bikes in Audi dataset
        "person": ["person", "moving-person"],
        "road": ["road", "lane-marking"],
        "parking": ["parking"],
        "sidewalk": ["sidewalk"],
        "building": ["building"],
        "nature": ["vegetation", "trunk", "terrain"],
        "other-objects": ["fence", "pole", "traffic-sign", "other-object"],
    }

    def __init__(self, split, preprocess_dir, merge_classes=False, pselab_paths=None):

        self.split = split
        self.preprocess_dir = preprocess_dir

        print("Initialize SemanticKITTI dataloader")

        # assert isinstance(split, tuple)
        print("Load", split)
        self.data = []
        for curr_split in split:
            with open(osp.join(self.preprocess_dir, curr_split + ".pkl"), "rb") as f:
                self.data.extend(pickle.load(f))

        self.pselab_data = None
        if pselab_paths:
            assert isinstance(pselab_paths, tuple)
            print("Load pseudo label data ", pselab_paths)
            self.pselab_data = []
            for curr_split in pselab_paths:
                self.pselab_data.extend(np.load(curr_split, allow_pickle=True))

            # check consistency of data and pseudo labels
            assert len(self.pselab_data) == len(self.data)
            for i in range(len(self.pselab_data)):
                assert len(self.pselab_data[i]["pseudo_label_2d"]) == len(
                    self.data[i]["seg_labels"]
                )

            # refine 2d pseudo labels
            probs2d = np.concatenate([data["probs_2d"] for data in self.pselab_data])
            pseudo_label_2d = np.concatenate(
                [data["pseudo_label_2d"] for data in self.pselab_data]
            ).astype(np.int)
            pseudo_label_2d = refine_pseudo_labels(probs2d, pseudo_label_2d)

            # refine 3d pseudo labels
            # fusion model has only one final prediction saved in probs_2d
            if "probs_3d" in self.pselab_data[0].keys():
                probs3d = np.concatenate(
                    [data["probs_3d"] for data in self.pselab_data]
                )
                pseudo_label_3d = np.concatenate(
                    [data["pseudo_label_3d"] for data in self.pselab_data]
                ).astype(np.int)
                pseudo_label_3d = refine_pseudo_labels(probs3d, pseudo_label_3d)
            else:
                pseudo_label_3d = None

            # undo concat
            left_idx = 0
            for data_idx in range(len(self.pselab_data)):
                right_idx = left_idx + len(self.pselab_data[data_idx]["probs_2d"])
                self.pselab_data[data_idx]["pseudo_label_2d"] = pseudo_label_2d[
                    left_idx:right_idx
                ]
                if pseudo_label_3d is not None:
                    self.pselab_data[data_idx]["pseudo_label_3d"] = pseudo_label_3d[
                        left_idx:right_idx
                    ]
                else:
                    self.pselab_data[data_idx]["pseudo_label_3d"] = None
                left_idx = right_idx

        if merge_classes:
            highest_id = list(self.id_to_class_name.keys())[-1]
            self.label_mapping = -100 * np.ones(highest_id + 2, dtype=int)
            for cat_idx, cat_list in enumerate(self.categories.values()):
                for class_name in cat_list:
                    self.label_mapping[self.class_name_to_id[class_name]] = cat_idx
            self.class_names = list(self.categories.keys())
        else:
            self.label_mapping = None

    def __getitem__(self, index):
        raise NotImplementedError

    def __len__(self):
        return len(self.data)


class SemanticKITTISCN(SemanticKITTIBase):
    def __init__(
        self,
        split,
        preprocess_dir,
        semantic_kitti_dir="",
        pselab_paths=None,
        merge_classes=True,
        scale=20,
        full_scale=4096,
        image_normalizer=None,
        noisy_rot=0.0,  # 3D augmentation
        flip_x=0.0,  # 3D augmentation
        rot_y=0.0,  # 3D augmentation
        transl=False,  # 3D augmentation
        bottom_crop=(),  # 2D augmentation (also effects 3D)
        fliplr=0.0,  # 2D augmentation
        color_jitter=None,  # 2D augmentation
        output_orig=False,
        use_image=True,
        resize=None,
    ):
        super().__init__(
            split,
            preprocess_dir,
            merge_classes=merge_classes,
            pselab_paths=pselab_paths,
        )

        self.semantic_kitti_dir = semantic_kitti_dir
        self.output_orig = output_orig

        # point cloud parameters
        self.scale = scale
        self.full_scale = full_scale
        # 3D augmentation
        self.noisy_rot = noisy_rot
        self.flip_x = flip_x
        self.rot_y = rot_y
        self.transl = transl

        # image parameters
        self.image_normalizer = image_normalizer
        # 2D augmentation
        self.bottom_crop = bottom_crop
        self.fliplr = fliplr
        self.color_jitter = T.ColorJitter(*color_jitter) if color_jitter else None

    def __getitem__(self, index):
        data_dict = self.data[index]

        pts_cam_coord = data_dict["pts_cam_coord"].copy()
        points = data_dict["points"].copy()
        seg_label = data_dict["seg_labels"].astype(np.int64)
        intrinsics = data_dict["intrinsics"].copy()

        if self.label_mapping is not None:
            seg_label = self.label_mapping[seg_label]

        keep_idx = np.ones(len(points), dtype=np.bool)
        points_img = data_dict["points_img"].copy()
        img_path = osp.join(self.semantic_kitti_dir, data_dict["camera_path"])
        image = Image.open(img_path)
        out_dict = {"path": img_path}

        # if self.split[0] == "train":
        #     depth_dense = np.load(
        #         img_path.replace("image_2", "depth").replace(".png", ".npy")
        #     )

        if self.bottom_crop:
            # self.bottom_crop is a tuple (crop_width, crop_height)
            left = int(np.random.rand() * (image.size[0] + 1 - self.bottom_crop[0]))
            right = left + self.bottom_crop[0]
            top = image.size[1] - self.bottom_crop[1]
            bottom = image.size[1]

            # update intrinsics
            intrinsics[0, 2] -= top
            intrinsics[1, 2] -= left

            # update image points
            keep_idx = points_img[:, 0] >= top
            keep_idx = np.logical_and(keep_idx, points_img[:, 0] < bottom)
            keep_idx = np.logical_and(keep_idx, points_img[:, 1] >= left)
            keep_idx = np.logical_and(keep_idx, points_img[:, 1] < right)

            # crop image
            image = image.crop((left, top, right, bottom))
            # if self.split[0] == "train":
            #     depth_dense = depth_dense[0][top:bottom, left:right]
            points_img = points_img[keep_idx]
            points_img[:, 0] -= top
            points_img[:, 1] -= left

            # update point cloud
            points = points[keep_idx]
            pts_cam_coord = pts_cam_coord[keep_idx]
            seg_label = seg_label[keep_idx]

        img_indices = points_img.astype(np.int64)
        depth = np.zeros((image.size[1], image.size[0]))
        depth[img_indices[:, 0], img_indices[:, 1]] = pts_cam_coord[:, 2]
        # depth_sparse = np.zeros((image.size[1], image.size[0]))

        # if self.split[0] == "train":
        # mask = np.random.rand(pts_cam_coord[:, 2].shape[0]) < 0.1
        # depth_sparse[
        #     img_indices[:, 0][mask], img_indices[:, 1][mask]
        # ] = pts_cam_coord[:, 2][mask]

        # mask = depth > 0
        # x_indeces_above_zero, y_indeces_above_zero = np.where(mask == True)
        # keep = np.random.choice(
        #     len(x_indeces_above_zero), (len(x_indeces_above_zero),), replace=False
        # )
        # keep = keep[:500]
        # depth_sparse = np.zeros_like(depth)
        # depth_sparse[
        #     x_indeces_above_zero[keep], y_indeces_above_zero[keep]
        # ] = depth[x_indeces_above_zero[keep], y_indeces_above_zero[keep]]

        #     mask = depth_dense > 0
        #     x_indeces_above_zero, y_indeces_above_zero = np.where(mask == True)
        #     keep = np.random.choice(
        #         len(x_indeces_above_zero), (len(x_indeces_above_zero),), replace=False
        #     )
        #     keep = keep[:4000]
        #     depth_sparse = np.zeros_like(depth_dense)
        #     depth_sparse[
        #         x_indeces_above_zero[keep], y_indeces_above_zero[keep]
        #     ] = depth_dense[x_indeces_above_zero[keep], y_indeces_above_zero[keep]]
        # else:
        depth_sparse = np.zeros_like(depth)
        depth_sparse[img_indices[:, 0], img_indices[:, 1]] = pts_cam_coord[:, 2]

        # depth_sparse = sample_lidar_lines(depth[..., None], intrinsics, keep_ratio=0.5)[
        # ..., 0
        # ]

        seg_labels_2d = np.ones((image.size[1], image.size[0])) * (-100)
        seg_labels_2d[img_indices[:, 0], img_indices[:, 1]] = seg_label

        # 2D augmentation
        if self.color_jitter is not None:
            image = self.color_jitter(image)
        # PIL to numpy
        image = np.array(image, dtype=np.float32, copy=False) / 255.0
        # 2D augmentation
        if np.random.rand() < self.fliplr:
            image = np.ascontiguousarray(np.fliplr(image))
            depth = np.ascontiguousarray(np.fliplr(depth))
            depth_sparse = np.ascontiguousarray(np.fliplr(depth_sparse))
            seg_labels_2d = np.ascontiguousarray(np.fliplr(seg_labels_2d))
            img_indices[:, 1] = image.shape[1] - 1 - img_indices[:, 1]
            intrinsics[0, 2] = image.shape[1] - intrinsics[0, 2]
            intrinsics[1, 2] = image.shape[0] - intrinsics[0, 1]

        # normalize image
        if self.image_normalizer:
            mean, std = self.image_normalizer
            mean = np.asarray(mean, dtype=np.float32)
            std = np.asarray(std, dtype=np.float32)
            image = (image - mean) / std

        out_dict["intrinsics"] = intrinsics
        out_dict["img"] = np.moveaxis(image, -1, 0)
        out_dict["img_indices"] = img_indices
        out_dict["depth"] = depth[None].astype(np.float32)
        out_dict["depth_sparse"] = depth_sparse[None].astype(np.float32)

        # 3D data augmentation and scaling from points to voxel indices
        # Kitti lidar coordinates: x (front), y (left), z (up)
        coords, min_value, offset, rot_matrix = augment_and_scale_3d(
            pts_cam_coord,
            self.scale,
            self.full_scale,
            noisy_rot=self.noisy_rot,
            flip_x=self.flip_x,
            rot_y=self.rot_y,
            transl=self.transl,
        )

        # cast to integer
        coords = coords.astype(np.int64)

        # only use voxels inside receptive field
        idxs = (coords.min(1) >= 0) * (coords.max(1) < self.full_scale)

        out_dict["coords"] = coords[idxs]
        out_dict["points"] = points[idxs]
        out_dict["min_value"] = min_value
        out_dict["offset"] = offset
        out_dict["rot_matrix"] = rot_matrix
        out_dict["pts_cam_coord"] = pts_cam_coord[idxs]
        out_dict["feats"] = np.ones(
            [len(idxs), 1], np.float32
        )  # simply use 1 as feature
        out_dict["seg_label"] = seg_label[idxs]
        out_dict["seg_labels_2d"] = seg_labels_2d
        out_dict["img_indices"] = out_dict["img_indices"][idxs]

        if self.pselab_data is not None:
            out_dict.update(
                {
                    "pseudo_label_2d": self.pselab_data[index]["pseudo_label_2d"][
                        keep_idx
                    ][idxs],
                    "pseudo_label_3d": self.pselab_data[index]["pseudo_label_3d"][
                        keep_idx
                    ][idxs],
                }
            )

        if self.output_orig:
            out_dict.update(
                {
                    "orig_seg_label": seg_label,
                    "orig_points_idx": idxs,
                }
            )

        return out_dict
