import json
import os.path as osp
import pickle

import numpy as np
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms as T

from lib.utils.augmentation_3d import augment_and_scale_3d


def sample_lidar_lines(
    depth_map: np.ndarray, intrinsics: np.ndarray, keep_ratio: float = 1.0
) -> np.ndarray:
    """
    Takes in input a depth map generated by a 64 line lidar and sparsify the number of
    lines used, returning a sparse depth map with less lidar lines.
    Parameters
    ----------
    depth_map: array like
        sparse depth map of shape H x W x 1
    intrinsics: array like
        the intrinsic parameters of shape 3 x 3
    keep_ratio: float, default 1.0
        the sparsification parameter, 1.0 is 64 lines, 0.50 roughly 32 lines and so on.
    Returns
    -------
    sparse_depth_map: array like
        the sparsified depth map of shape H x W x 1
    """

    v, u, _ = np.nonzero(depth_map)
    z = depth_map[v, u, 0]
    points = np.linalg.inv(intrinsics) @ (np.vstack([u, v, np.ones_like(u)]) * z)
    points = points.transpose([1, 0])

    scan_y = points[:, 1]
    distance = np.linalg.norm(points, 2, axis=1)
    pitch = np.arcsin(scan_y / distance)
    num_points = np.shape(pitch)[0]
    pitch = np.reshape(pitch, (num_points, 1))

    max_pitch = np.max(pitch)
    min_pitch = np.min(pitch)
    angle_interval = (max_pitch - min_pitch) / 64.0
    angle_label = np.round((pitch - min_pitch) / angle_interval)
    sampling_mask = angle_label % (1.0 / keep_ratio) == 0

    final_mask = np.zeros_like(depth_map, dtype=bool)
    final_mask[depth_map[..., 0] > 0] = sampling_mask
    sampled_depth = np.zeros_like(final_mask, dtype=np.float32)
    sampled_depth[final_mask] = depth_map[final_mask]
    return sampled_depth


class A2D2Base(Dataset):
    """A2D2 dataset"""

    class_names = [
        "Car 1",
        "Car 2",
        "Car 3",
        "Car 4",
        "Bicycle 1",
        "Bicycle 2",
        "Bicycle 3",
        "Bicycle 4",
        "Pedestrian 1",
        "Pedestrian 2",
        "Pedestrian 3",
        "Truck 1",
        "Truck 2",
        "Truck 3",
        "Small vehicles 1",
        "Small vehicles 2",
        "Small vehicles 3",
        "Traffic signal 1",
        "Traffic signal 2",
        "Traffic signal 3",
        "Traffic sign 1",
        "Traffic sign 2",
        "Traffic sign 3",
        "Utility vehicle 1",
        "Utility vehicle 2",
        "Sidebars",
        "Speed bumper",
        "Curbstone",
        "Solid line",
        "Irrelevant signs",
        "Road blocks",
        "Tractor",
        "Non-drivable street",
        "Zebra crossing",
        "Obstacles / trash",
        "Poles",
        "RD restricted area",
        "Animals",
        "Grid structure",
        "Signal corpus",
        "Drivable cobblestone",
        "Electronic traffic",
        "Slow drive area",
        "Nature object",
        "Parking area",
        "Sidewalk",
        "Ego car",
        "Painted driv. instr.",
        "Traffic guide obj.",
        "Dashed line",
        "RD normal street",
        "Sky",
        "Buildings",
        "Blurred area",
        "Rain dirt",
    ]

    # use those categories if merge_classes == True
    categories = {
        "car": ["Car 1", "Car 2", "Car 3", "Car 4", "Ego car"],
        "truck": ["Truck 1", "Truck 2", "Truck 3"],
        "bike": [
            "Bicycle 1",
            "Bicycle 2",
            "Bicycle 3",
            "Bicycle 4",
            "Small vehicles 1",
            "Small vehicles 2",
            "Small vehicles 3",
        ],  # small vehicles are "usually" motorcycles
        "person": ["Pedestrian 1", "Pedestrian 2", "Pedestrian 3"],
        "road": [
            "RD normal street",
            "Zebra crossing",
            "Solid line",
            "RD restricted area",
            "Slow drive area",
            "Drivable cobblestone",
            "Dashed line",
            "Painted driv. instr.",
        ],
        "parking": ["Parking area"],
        "sidewalk": ["Sidewalk", "Curbstone"],
        "building": ["Buildings"],
        "nature": ["Nature object"],
        "other-objects": [
            "Poles",
            "Traffic signal 1",
            "Traffic signal 2",
            "Traffic signal 3",
            "Traffic sign 1",
            "Traffic sign 2",
            "Traffic sign 3",
            "Sidebars",
            "Speed bumper",
            "Irrelevant signs",
            "Road blocks",
            "Obstacles / trash",
            "Animals",
            "Signal corpus",
            "Electronic traffic",
            "Traffic guide obj.",
            "Grid structure",
        ],
        # 'ignore': ['Sky', 'Utility vehicle 1', 'Utility vehicle 2', 'Tractor', 'Non-drivable street',
        #            'Blurred area', 'Rain dirt'],
    }

    def __init__(self, split, preprocess_dir, merge_classes=True):

        self.split = split
        self.preprocess_dir = preprocess_dir

        print("Initialize A2D2 dataloader")

        with open(osp.join(self.preprocess_dir, "cams_lidars.json"), "r") as f:
            self.config = json.load(f)

        # assert isinstance(split, tuple)
        print("Load", split)
        self.data = []
        for curr_split in split:
            with open(osp.join(self.preprocess_dir, curr_split + ".pkl"), "rb") as f:
                self.data.extend(pickle.load(f))

        with open(osp.join(self.preprocess_dir, "class_list.json"), "r") as f:
            class_list = json.load(f)
            self.rgb_to_class = {}
            self.rgb_to_cls_idx = {}
            count = 0
            for k, v in class_list.items():
                # hex to rgb
                rgb_value = tuple(int(k.lstrip("#")[i : i + 2], 16) for i in (0, 2, 4))
                self.rgb_to_class[rgb_value] = v
                self.rgb_to_cls_idx[rgb_value] = count
                count += 1

        assert self.class_names == list(self.rgb_to_class.values())
        if merge_classes:
            self.label_mapping = -100 * np.ones(len(self.rgb_to_class) + 1, dtype=int)
            for cat_idx, cat_list in enumerate(self.categories.values()):
                for class_name in cat_list:
                    self.label_mapping[self.class_names.index(class_name)] = cat_idx
            self.class_names = list(self.categories.keys())
        else:
            self.label_mapping = None

    def __getitem__(self, index):
        raise NotImplementedError

    def __len__(self):
        return len(self.data)


class A2D2SCN(A2D2Base):
    def __init__(
        self,
        split,
        preprocess_dir,
        merge_classes=True,
        scale=20,
        full_scale=4096,
        use_image=True,
        resize=(480, 302),
        image_normalizer=None,
        noisy_rot=0.0,  # 3D augmentation
        flip_x=0.0,  # 3D augmentation
        rot_y=0.0,  # 3D augmentation
        transl=False,  # 3D augmentation
        fliplr=0.0,  # 2D augmentation
        color_jitter=None,  # 2D augmentation
        bottom_crop=None,
    ):
        super().__init__(split, preprocess_dir, merge_classes=merge_classes)

        # point cloud parameters
        self.scale = scale
        self.full_scale = full_scale
        # 3D augmentation
        self.noisy_rot = noisy_rot
        self.flip_x = flip_x
        self.rot_y = rot_y
        self.transl = transl

        # image parameters
        self.use_image = use_image
        if self.use_image:
            self.resize = resize
            self.image_normalizer = image_normalizer

            # data augmentation
            self.fliplr = fliplr
            self.color_jitter = T.ColorJitter(*color_jitter) if color_jitter else None

    def __getitem__(self, index):
        data_dict = self.data[index]
        pts_cam_coord = data_dict["pts_cam_coord"].copy()
        # if pts_cam_coord.shape[0] < 1000:
        #     data_dict = self.data[0]
        #     pts_cam_coord = data_dict["pts_cam_coord"].copy()

        points = data_dict["points"].copy()
        seg_label = data_dict["seg_labels"].astype(np.int64)
        intrinsics = np.array(
            [
                [1687.3369140625, 0.0, 965.43414055823814],
                [0.0, 1783.428466796875, 684.4193604186803],
                [0.0, 0.0, 1.0],
            ]
        ).copy()
        intrinsics[:2] /= 4

        if self.label_mapping is not None:
            seg_label = self.label_mapping[seg_label]

        if self.use_image:
            points_img = data_dict["points_img"].copy()
            img_path = osp.join(
                self.preprocess_dir,
                "preprocessed_camera_frame",
                data_dict["camera_path"],
            )
            image = Image.open(img_path)

            if self.resize:
                if not image.size == self.resize:
                    # check if we do not enlarge downsized images
                    assert image.size[0] > self.resize[0]

                    # scale image points
                    points_img[:, 0] = (
                        float(self.resize[1])
                        / image.size[1]
                        * np.floor(points_img[:, 0])
                    )
                    points_img[:, 1] = (
                        float(self.resize[0])
                        / image.size[0]
                        * np.floor(points_img[:, 1])
                    )

                    # resize image
                    image = image.resize(self.resize, Image.BILINEAR)

            img_indices = points_img.astype(np.int64)

            depth = np.zeros((image.size[1], image.size[0]))
            depth[img_indices[:, 0], img_indices[:, 1]] = pts_cam_coord[:, 2]

            # depth_dense = np.load(
            #     img_path.replace("cam_front_center", "depth").replace(".png", ".npy")
            # )[0]

            # mask = depth_dense > 0
            # x_indeces_above_zero, y_indeces_above_zero = np.where(mask == True)
            # keep = np.random.choice(
            #     len(x_indeces_above_zero), (len(x_indeces_above_zero),), replace=False
            # )
            # keep = keep[:4000]
            depth_sparse = np.zeros_like(depth)
            # depth_sparse[
            #     x_indeces_above_zero[keep], y_indeces_above_zero[keep]
            # ] = depth[x_indeces_above_zero[keep], y_indeces_above_zero[keep]]

            depth_sparse[img_indices[:, 0], img_indices[:, 1]] = pts_cam_coord[:, 2]

            # depth_sparse = sample_lidar_lines(
            # depth[..., None], intrinsics, keep_ratio=0.8
            # )[..., 0]
            # depth_sparse = np.zeros((image.size[1], image.size[0]))
            # mask = np.random.rand(seg_label.shape[0]) < 0.4
            # depth_sparse[
            #     img_indices[:, 0][mask], img_indices[:, 1][mask]
            # ] = pts_cam_coord[:, 2][mask]

            seg_labels_2d = np.ones((image.size[1], image.size[0])) * (-100)
            seg_labels_2d[img_indices[:, 0], img_indices[:, 1]] = seg_label

            assert np.all(img_indices[:, 0] >= 0)
            assert np.all(img_indices[:, 1] >= 0)
            assert np.all(img_indices[:, 0] < image.size[1])
            assert np.all(img_indices[:, 1] < image.size[0])

            # 2D augmentation
            if self.color_jitter is not None:
                image = self.color_jitter(image)
            # PIL to numpy
            image = np.array(image, dtype=np.float32, copy=False) / 255.0
            # 2D augmentation
            if np.random.rand() < self.fliplr:
                image = np.ascontiguousarray(np.fliplr(image))
                depth = np.ascontiguousarray(np.fliplr(depth))
                depth_sparse = np.ascontiguousarray(np.fliplr(depth_sparse))
                seg_labels_2d = np.ascontiguousarray(np.fliplr(seg_labels_2d))
                img_indices[:, 1] = image.shape[1] - 1 - img_indices[:, 1]
                intrinsics[0, 2] = image.shape[1] - intrinsics[0, 2]
                intrinsics[1, 2] = image.shape[0] - intrinsics[0, 1]

            # normalize image
            if self.image_normalizer:
                mean, std = self.image_normalizer
                mean = np.asarray(mean, dtype=np.float32)
                std = np.asarray(std, dtype=np.float32)
                image = (image - mean) / std

            out_dict = {"path": img_path}
            out_dict["intrinsics"] = intrinsics
            out_dict["img"] = np.moveaxis(image, -1, 0)
            out_dict["img_indices"] = img_indices
            out_dict["depth"] = depth[None].astype(np.float32)
            out_dict["depth_sparse"] = depth_sparse[None].astype(np.float32)

        # 3D data augmentation and scaling from points to voxel indices
        # A2D2 lidar coordinates (same as Kitti): x (front), y (left), z (up)
        coords, min_value, offset, rot_matrix = augment_and_scale_3d(
            pts_cam_coord,
            self.scale,
            self.full_scale,
            noisy_rot=self.noisy_rot,
            flip_x=self.flip_x,
            rot_y=self.rot_y,
            transl=self.transl,
        )

        # cast to integer
        coords = coords.astype(np.int64)

        # only use voxels inside receptive field
        idxs = (coords.min(1) >= 0) * (coords.max(1) < self.full_scale)

        out_dict["coords"] = coords[idxs]
        out_dict["points"] = points[idxs]
        out_dict["min_value"] = min_value
        out_dict["offset"] = offset
        out_dict["rot_matrix"] = rot_matrix
        out_dict["pts_cam_coord"] = pts_cam_coord[idxs]
        out_dict["feats"] = np.ones(
            [len(idxs), 1], np.float32
        )  # simply use 1 as feature
        out_dict["seg_label"] = seg_label[idxs]
        out_dict["seg_labels_2d"] = seg_labels_2d

        if self.use_image:
            out_dict["img_indices"] = out_dict["img_indices"][idxs]

        return out_dict
